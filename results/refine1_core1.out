
# ------------------------------------------------------------------------------
Execution of code_aster

# ------------------------------------------------------------------------------
Prepare environment in /tmp/run_aster_q4b16j_k/proc.0

# ------------------------------------------------------------------------------
Command file #1 / 1

Content of the file to execute:
# coding=utf-8
#!/usr/bin/python

import os
from statistics import mean
from datetime import datetime
from resource import RUSAGE_SELF, getrusage

from code_aster.Commands import *
from code_aster import CA
from code_aster.Utilities import petscInitialize

CA.init()

params = {}
params["refinements"] = int(os.environ.get("REFINE", 1))
params["parallel"] = os.environ.get("USE_LEGACY", "HPC")
params["solver"] = os.environ.get("SOLVER", "PETSC")

# General parameters
comm = CA.MPI.ASTER_COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

nbHexa = 8 ** params["refinements"]


def memory_peak(mess=None):
    """Return memory peak in MB"""
    return int(getrusage(RUSAGE_SELF).ru_maxrss / 1024)


class ChronoCtxMgGen:
    stats = {}

    def __init__(self, what):
        self._what = what

    def __enter__(self):
        self.start = datetime.now()

    def __exit__(self, exctype, exc, tb):
        self.stop = datetime.now()
        delta = self.stop - self.start
        mem = memory_peak(self._what)
        self.stats[self._what] = [delta.total_seconds(), mem]


class ChronoCtxMg(ChronoCtxMgGen):
    pass
    # def __init__(self, what):
    #     ChronoCtxMgGen.__init__(self, what)


def write_stats(nume_ddl):
    if rank == 0:
        print("TITLE: TEST PERF CUBE")
        print()
        print("NB PROC")
        print(size)
        print()
        print(
            "COMMAND, TIME MIN (s), TIME MAX (s), TIME MEAN (s), MEM MIN (Mo), MEM MAX (Mo), MEM MEAN (Mo)"
        )

    for key, values in stats.items():
        time = comm.gather(values[0], root=0)
        mem = comm.gather(values[1], root=0)
        if rank == 0:
            print(
                key
                + ", "
                + str(min(time))
                + ", "
                + str(max(time))
                + ", "
                + str(mean(time))
                + ", "
                + str(min(mem))
                + ", "
                + str(max(mem))
                + ", "
                + str(mean(mem))
            )

    mesh = nume_ddl.getMesh()
    nodes = len(mesh.getInnerNodes())
    nodes = comm.allreduce(nodes, CA.MPI.SUM)

    if rank == 0:
        print()
        print("NB CELLS, NB NODES, NB DOFS")
        print(str(nbHexa) + ", " + str(nodes) + ", " + str(nume_ddl.getNumberOfDofs()))


def print_markdown_table(data, refine, nbcells, nbnodes, nbdofs):
    """Print a table of the mean time as a Markdown table."""

    def show(*args, **kwargs):
        if rank == 0:
            print(*args, **kwargs)

    fmti = "| {0:<16s} | {1:11,d} |"
    fmtt = "| {0:<16s} | {1:11.2f} |"
    separ = "| :--------------- | ----------: |"
    show(fmti.format("Refinement", refine))
    show(separ)
    show(fmti.format("Number of cells", nbcells).replace(",", " "))
    show(fmti.format("Number of nodes", nbnodes).replace(",", " "))
    show(fmti.format("Number of DOFs", nbdofs).replace(",", " "))
    show(fmti.format("Number of procs", size).replace(",", " "))
    show(fmti.format("Nb of DOFs/proc", nbdofs // size).replace(",", " "))
    for key, values in data.items():
        times = comm.gather(values[0], root=0)
        # mem = comm.gather(values[1], root=0)
        if rank == 0:
            show(fmtt.format(key, mean(times)))


# petscInitialize('-ksp_monitor_true_residual -stats' )
petscInitialize("-ksp_monitor_true_residual -log_view")

with ChronoCtxMg("Total"):
    with ChronoCtxMg("Build mesh"):
        if params["parallel"] == "HPC":
            mesh = CA.ParallelMesh.buildCube(refine=params["refinements"])
        else:
            mesh = CA.Mesh.buildCube(refine=params["refinements"])

    with ChronoCtxMg("Model"):
        model = AFFE_MODELE(
            MAILLAGE=mesh,
            AFFE=_F(
                TOUT="OUI",
                PHENOMENE="MECANIQUE",
                MODELISATION="3D",
            ),
        )

    with ChronoCtxMg("Material"):
        steel = DEFI_MATERIAU(
            ELAS=_F(
                E=200000.0,
                NU=0.3,
            ),
            ECRO_LINE=_F(
                D_SIGM_EPSI=2000.0,
                SY=200.0,
            ),
        )

        mater = AFFE_MATERIAU(
            MAILLAGE=mesh,
            AFFE=_F(
                TOUT="OUI",
                MATER=steel,
            ),
        )

    with ChronoCtxMg("Boundary conditions"):
        block = AFFE_CHAR_CINE(
            MODELE=model,
            MECA_IMPO=(
                _F(
                    GROUP_MA="LEFT",
                    DX=0,
                    DY=0.0,
                    DZ=0.0,
                ),
            ),
        )

        imposed_displ = AFFE_CHAR_CINE(
            MODELE=model,
            MECA_IMPO=(
                _F(
                    GROUP_MA="RIGHT",
                    DY=0.001,
                    DZ=0.001,
                ),
            ),
        )

    with ChronoCtxMg("Create matrix"):
        stiff_elem = CALC_MATR_ELEM(
            MODELE=model,
            OPTION="RIGI_MECA",
            CHAM_MATER=mater,
        )

    with ChronoCtxMg("Numbering"):
        dofNum = NUME_DDL(
            MATR_RIGI=stiff_elem,
        )

    with ChronoCtxMg("Assembly"):
        stiffness = ASSE_MATRICE(
            MATR_ELEM=stiff_elem,
            NUME_DDL=dofNum,
            CHAR_CINE=(block, imposed_displ),
        )

    with ChronoCtxMg("Build RHS"):
        rhs = CREA_CHAMP(
            TYPE_CHAM="NOEU_DEPL_R",
            OPERATION="AFFE",
            MAILLAGE=mesh,
            AFFE=_F(
                TOUT="OUI",
                NOM_CMP=(
                    "DX",
                    "DY",
                    "DZ",
                ),
                VALE=(
                    0.0,
                    0.0,
                    0.0,
                ),
            ),
        )

        load_vector = CALC_CHAR_CINE(NUME_DDL=dofNum, CHAR_CINE=(block, imposed_displ))

    if params["solver"] == "PETSC":
        solver = CA.PetscSolver(RENUM="SANS", PRE_COND="GAMG")
    elif params["solver"] == "MUMPS":
        solver = CA.MumpsSolver(
            MATR_DISTRIBUEE="OUI",
            RENUM="PARMETIS",
            ACCELERATION="FR+",
            POSTTRAITEMENTS="MINI",
        )

    with ChronoCtxMg("Factorize"):
        solver.factorize(stiffness)

    with ChronoCtxMg("Solve"):
        resu = solver.solve(rhs, load_vector)

# write_stats(dofNum)
nbNodes = len(mesh.getInnerNodes())
if params["parallel"] == "HPC":
    nbNodes = comm.allreduce(nbNodes, CA.MPI.SUM)
nbDOFs = dofNum.getNumberOfDOFs()
print_markdown_table(ChronoCtxMg.stats, params["refinements"], nbHexa, nbNodes, nbDOFs)

CA.close()



# ------------------------------------------------------------------------------
Command line #1:
    ulimit -c unlimited ; ulimit -t 108000 ; ( /fsx/home/etud8-4/codeaster-prerequisites-20240327-oss/venv/bin/python3 -m mpi4py /fsx/home/etud8-4/opensource-installation-development/benchmarks/Cube_files/Cube_perf.py --last --tpmax 86400 ; echo $? > _exit_code_ ) 2>&1 | tee -a fort.6
setting '--memory' value to 3686.40 MB (keyword RESERVE_MEMOIRE)
checking MPI initialization...
using COMM_WORLD.
MPI is initialized.
Ouverture en écriture du fichier ./vola.1

<INFO> Démarrage de l'exécution.

                       -- CODE_ASTER -- VERSION : DÉVELOPPEMENT (unstable) --                       
                               Version 17.2.5 modifiée le 23/01/2025                                
                               révision 9ebef44bf765 - branche 'main'                               
                                   Copyright EDF R&D 1991 - 2025                                    
                                                                                                    
                              Exécution du : Fri Jan 24 16:35:35 2025                               
                             Nom de la machine : c8g-st-c8g-24xlarge-1                              
                                        Architecture : 64bit                                        
                                    Type de processeur : aarch64                                    
        Système d'exploitation : Linux-5.10.226-214.880.amzn2.aarch64-aarch64-with-glibc2.17        
                                  Langue des messages : en (UTF-8)                                  
                                     Version de Python : 3.7.16                                     
                                     Version de NumPy : 1.21.6                                      
                                      Parallélisme MPI : actif                                      
                                   Rang du processeur courant : 0                                   
                               Nombre de processeurs MPI utilisés : 1                               
                                    Parallélisme OpenMP : actif                                     
                              Nombre de processus OpenMP utilisés : 1                               
                               Version de la librairie HDF5 : 1.10.9                                
                                Version de la librairie MED : 4.1.1                                 
                               Version de la librairie MFront : 4.2.0                               
                               Version de la librairie MUMPS : 5.6.2                                
                              Version de la librairie PETSc : 3.20.5p0                              
                               Version de la librairie SCOTCH : 7.0.4                               

starting the execution...
Valeur initiale du temps CPU maximum =   86400 secondes
  Valeur du temps CPU maximum passé aux commandes =   77760 secondes
  Réserve CPU prévue = 8640 secondes

Ouverture en écriture du fichier ./glob.1

Ouverture en écriture du fichier ./vola.1

Ouverture en lecture du fichier /fsx/home/etud8-4/aster/install/mpi/lib64/aster/elem.1

Nom de la base                          :  ELEMBASE
     Créée avec la version                   :  17.02.05
     Nombre d'enregistrements utilisés       :  45
     Nombre d'enregistrements maximum        :  512
     Nombre d'enregistrements par fichier    :  512
     Longueur d'enregistrement (octets)      :  819200
     Nombre d'identificateurs utilisés       :  123
     Taille maximum du répertoire            :  300
     Pourcentage d'utilisation du répertoire :  41 %

Ouverture en lecture du fichier /fsx/home/etud8-4/aster/install/mpi/lib64/aster/elem.1

Nom de la base                          :  ELEMBASE
     Nombre d'enregistrements utilisés       :  45
     Nombre d'enregistrements maximum        :  512
     Nombre d'enregistrements par fichier    :  512
     Longueur d'enregistrement (octets)      :  819200
     Nombre total d'accès en lecture         :  63
     Volume des accès en lecture             :         49.22 Mo.
     Nombre total d'accès en écriture        :  0
     Volume des accès en écriture            :          0.00 Mo.
     Nombre d'identificateurs utilisés       :  123
     Taille maximum du répertoire            :  300
     Pourcentage d'utilisation du répertoire :  41 %

Relecture des catalogues des éléments faite.

Fin de lecture (durée  0.022790  s.) 

                      Mémoire limite pour l'allocation dynamique : 4219.37 Mo                       
                         ajouté à l'initialisation du processus : 739.77 Mo                         
                               Limite cible du processus : 4959.13 Mo                               
                         Taille limite des fichiers d'échange : 2048.00 Go                          
# Mémoire (Mo) :   739.77 /   730.88 /   209.22 /   185.03 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0001   user+syst:        0.01s (syst:        0.12s, elaps:        0.13s)
# ----------------------------------------------------------------------------------------------
PETSc initialized...
Nom MED du maillage : PARALLEPIPED


------------ MAILLAGE 00000001 - IMPRESSIONS NIVEAU  1 ------------


NOMBRE DE NOEUDS                          27

NOMBRE DE MAILLES                         56
                              SEG2                   24
                              QUAD4                  24
                              HEXA8                   8

NOMBRE DE GROUPES DE NOEUDS                8

NOMBRE DE GROUPES DE MAILLES              19

--------------------------------------------------------------------------------


.. _stg1_txt190
# ----------------------------------------------------------------------------------------------
# Commande #0002 de
/fsx/home/etud8-4/aster/install/mpi/lib64/aster/code_aster/Helpers/LogicalUnit.py, ligne 190
DEFI_FICHIER(ACCES='NEW',
             ACTION='ASSOCIER',
             FICHIER='/tmp/buildCubem0gl_bx8/buildCube.med',
             TYPE='BINARY',
             UNITE=99)

Deleting '/tmp/buildCubem0gl_bx8/buildCube.med': No such file or directory
# Mémoire (Mo) :   766.75 /   762.75 /   209.22 /   185.03 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0002   user+syst:        0.00s (syst:        0.00s, elaps:        0.00s)
# ----------------------------------------------------------------------------------------------
Création du fichier au format MED 3.3.1.


.. _stg1_txt190
# ----------------------------------------------------------------------------------------------
# Commande #0003 de
/fsx/home/etud8-4/aster/install/mpi/lib64/aster/code_aster/Helpers/LogicalUnit.py, ligne 190
DEFI_FICHIER(ACTION='LIBERER',
             UNITE=99)

# Mémoire (Mo) :   766.75 /   763.52 /   209.22 /   185.03 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0003   user+syst:        0.00s (syst:        0.01s, elaps:        0.00s)
# ----------------------------------------------------------------------------------------------
<INFO> Activation du mode parallélisme distribué.

Nom MED du maillage : 00000001


--------------------------------------------------------------------------------


--------------------------------------------------------------------------------


.. _stg1_txt282
# ----------------------------------------------------------------------------------------------
# Commande #0004 de
/fsx/home/etud8-4/aster/install/mpi/lib64/aster/code_aster/ObjectsExt/parallelmesh_ext.py, ligne 282
CREA_MAILLAGE(INFO=1,
              MAILLAGE='<00000002>',
              RAFFINEMENT=_F(NIVEAU=0,
                             TOUT='OUI'))


------------ MAILLAGE 00000004 - IMPRESSIONS NIVEAU  1 ------------

ASTER 17.02.05 CONCEPT 00000004 CALCULE LE 24/01/2025 A 16:35:35 DE TYPE        
MAILLAGE_P                                                                      

NOMBRE DE NOEUDS                          27

NOMBRE DE MAILLES                         56
                              SEG2                   24
                              QUAD4                  24
                              HEXA8                   8

NOMBRE DE GROUPES DE NOEUDS                8

NOMBRE DE GROUPES DE MAILLES              19

--------------------------------------------------------------------------------

#4      Communications MPI                                CPU (USER+SYST/SYST/ELAPS):      0.00      0.00      0.00
# Résultat commande #0004 (CREA_MAILLAGE): '<00000004>' de type <ParallelMesh>
# Mémoire (Mo) :   770.86 /   766.86 /   209.22 /   185.03 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0004   user+syst:        0.00s (syst:        0.00s, elaps:        0.00s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt136
# ----------------------------------------------------------------------------------------------
# Commande #0005 de
/fsx/home/etud8-4/opensource-installation-development/benchmarks/Cube_files/Cube_perf.py, ligne 136
model = AFFE_MODELE(AFFE=_F(MODELISATION='3D',
                            PHENOMENE='MECANIQUE',
                            TOUT='OUI'),
                    DISTRIBUTION=_F(METHODE='CENTRALISE'),
                    INFO=1,
                    MAILLAGE='<00000004>',
                    VERI_JACOBIEN='OUI',
                    VERI_NORM_IFS='OUI',
                    VERI_PLAN='OUI')

Sur les 56 mailles du maillage 00000004, on a demandé l'affectation de 56, on a pu en affecter 56.
Modélisation     Formulation      Type maille  Élément fini     Nombre
_                _                SEG2         MECA_ARETE2      24
_                _                QUAD4        MECA_FACE4       24
3D               _                HEXA8        MECA_HEXA8       8
#2      Calculs elementaires et assemblages               CPU (USER+SYST/SYST/ELAPS):      0.00      0.00      0.00
#4      Communications MPI                                CPU (USER+SYST/SYST/ELAPS):      0.00      0.00      0.00
# Résultat commande #0005 (AFFE_MODELE): model ('<00000005>') de type <Model>
# Mémoire (Mo) :   799.54 /   799.54 /   209.22 /   197.88 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0005   user+syst:        0.01s (syst:        0.01s, elaps:        0.01s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt148
# ----------------------------------------------------------------------------------------------
# Commande #0006 de
/fsx/home/etud8-4/opensource-installation-development/benchmarks/Cube_files/Cube_perf.py, ligne 148
steel = DEFI_MATERIAU(ECRO_LINE=_F(D_SIGM_EPSI=2000.0,
                                   SY=200.0),
                      ELAS=_F(B_ENDOGE=0.0,
                              COEF_AMOR=1.0,
                              E=200000.0,
                              K_DESSIC=0.0,
                              NU=0.3),
                      INFO=1)

# Résultat commande #0006 (DEFI_MATERIAU): steel ('<00000006>') de type <Material>
# Mémoire (Mo) :   800.29 /   799.79 /   209.22 /   197.88 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0006   user+syst:        0.02s (syst:        0.00s, elaps:        0.03s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt156
# ----------------------------------------------------------------------------------------------
# Commande #0007 de
/fsx/home/etud8-4/opensource-installation-development/benchmarks/Cube_files/Cube_perf.py, ligne 156
mater = AFFE_MATERIAU(AFFE=_F(MATER=steel,
                              TOUT='OUI'),
                      INFO=1,
                      MAILLAGE='<00000004>')

# Résultat commande #0007 (AFFE_MATERIAU): mater ('<00000007>') de type <MaterialField>
# Mémoire (Mo) :   800.29 /   799.79 /   209.22 /   197.88 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0007   user+syst:        0.01s (syst:        0.00s, elaps:        0.01s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt168
# ----------------------------------------------------------------------------------------------
# Commande #0008 de
/fsx/home/etud8-4/opensource-installation-development/benchmarks/Cube_files/Cube_perf.py, ligne 168
block = AFFE_CHAR_CINE(INFO=1,
                       MECA_IMPO=_F(DX=0,
                                    DY=0.0,
                                    DZ=0.0,
                                    GROUP_MA='LEFT'),
                       MODELE=model,
                       SYNTAXE='NON')

# Résultat commande #0008 (AFFE_CHAR_CINE): block ('<00000008>') de type <MechanicalDirichletBC>
# Mémoire (Mo) :   800.29 /   799.79 /   209.22 /   197.88 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0008   user+syst:        0.01s (syst:        0.00s, elaps:        0.00s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt179
# ----------------------------------------------------------------------------------------------
# Commande #0009 de
/fsx/home/etud8-4/opensource-installation-development/benchmarks/Cube_files/Cube_perf.py, ligne 179
imposed_displ = AFFE_CHAR_CINE(INFO=1,
                               MECA_IMPO=_F(DY=0.001,
                                            DZ=0.001,
                                            GROUP_MA='RIGHT'),
                               MODELE=model,
                               SYNTAXE='NON')

# Résultat commande #0009 (AFFE_CHAR_CINE): imposed_displ ('<00000009>') de type
<MechanicalDirichletBC>
# Mémoire (Mo) :   800.29 /   799.79 /   209.22 /   197.88 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0009   user+syst:        0.00s (syst:        0.00s, elaps:        0.01s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt188
# ----------------------------------------------------------------------------------------------
# Commande #0010 de
/fsx/home/etud8-4/opensource-installation-development/benchmarks/Cube_files/Cube_perf.py, ligne 188
stiff_elem = CALC_MATR_ELEM(CALC_ELEM_MODELE='OUI',
                            CHAM_MATER=mater,
                            INST=0.0,
                            MODELE=model,
                            MODE_FOURIER=0,
                            OPTION='RIGI_MECA')

# Résultat commande #0010 (CALC_MATR_ELEM): stiff_elem ('<0000000b>') de type
<ElementaryMatrixDisplacementReal>
# Mémoire (Mo) :   800.29 /   799.80 /   209.22 /   197.88 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0010   user+syst:        0.02s (syst:        0.00s, elaps:        0.01s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt193
# ----------------------------------------------------------------------------------------------
# Commande #0011 de
/fsx/home/etud8-4/opensource-installation-development/benchmarks/Cube_files/Cube_perf.py, ligne 193
dofNum = NUME_DDL(INFO=1,
                  MATR_RIGI=stiff_elem)

Le système linéaire à résoudre a 81 degrés de liberté:
   - 81 sont des degrés de liberté physiques
     (ils sont portés par 27 noeuds du maillage)
   - 0 sont les couples de paramètres de Lagrange associés
     aux 0 relations linéaires dualisées.
La matrice est de taille 81 équations.
  Elle contient 1584 termes non nuls si elle est symétrique et 3087 termes non nuls si elle n'est
pas symétrique.
  Soit un taux de remplissage de  47.051 %.
# Résultat commande #0011 (NUME_DDL): dofNum ('<00000011>') de type <ParallelDOFNumbering>
# Mémoire (Mo) :   800.29 /   799.80 /   209.22 /   197.88 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0011   user+syst:        0.01s (syst:        0.00s, elaps:        0.01s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt200
# ----------------------------------------------------------------------------------------------
# Commande #0012 de
/fsx/home/etud8-4/opensource-installation-development/benchmarks/Cube_files/Cube_perf.py, ligne 200
stiffness = ASSE_MATRICE(CHAR_CINE=(block, imposed_displ),
                         INFO=1,
                         MATR_ELEM=stiff_elem,
                         NUME_DDL=dofNum)

# Résultat commande #0012 (ASSE_MATRICE): stiffness ('<00000013>') de type
<AssemblyMatrixDisplacementReal>
# Mémoire (Mo) :   800.29 /   799.80 /   209.22 /   197.88 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0012   user+syst:        0.01s (syst:        0.00s, elaps:        0.02s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt216
# ----------------------------------------------------------------------------------------------
# Commande #0013 de
/fsx/home/etud8-4/opensource-installation-development/benchmarks/Cube_files/Cube_perf.py, ligne 216
rhs = CREA_CHAMP(AFFE=_F(NOM_CMP=('DX', 'DY', 'DZ'),
                         TOUT='OUI',
                         VALE=(0.0, 0.0, 0.0)),
                 INFO=1,
                 MAILLAGE='<00000004>',
                 OPERATION='AFFE',
                 TYPE_CHAM='NOEU_DEPL_R')

#4      Communications MPI                                CPU (USER+SYST/SYST/ELAPS):      0.00      0.00      0.00
# Résultat commande #0013 (CREA_CHAMP): rhs ('<00000015>') de type <FieldOnNodesReal>
# Mémoire (Mo) :   800.29 /   799.80 /   209.22 /   197.88 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0013   user+syst:        0.01s (syst:        0.00s, elaps:        0.00s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt223
# ----------------------------------------------------------------------------------------------
# Commande #0014 de
/fsx/home/etud8-4/opensource-installation-development/benchmarks/Cube_files/Cube_perf.py, ligne 223
load_vector = CALC_CHAR_CINE(CHAR_CINE=(block, imposed_displ),
                             INFO=1,
                             INST=0.0,
                             NUME_DDL=dofNum)

# Résultat commande #0014 (CALC_CHAR_CINE): load_vector ('<00000017>') de type <FieldOnNodesReal>
# Mémoire (Mo) :   800.29 /   799.80 /   209.22 /   197.88 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0014   user+syst:        0.01s (syst:        0.00s, elaps:        0.02s)
# ----------------------------------------------------------------------------------------------
  0 KSP unpreconditioned resid norm 2.295641613145e+02 true resid norm 2.295641613145e+02 ||r(i)||/||b|| 1.000000000000e+00
  1 KSP unpreconditioned resid norm 2.455109942662e+01 true resid norm 2.455109942662e+01 ||r(i)||/||b|| 1.069465690378e-01
  2 KSP unpreconditioned resid norm 1.396788534436e+00 true resid norm 1.396788534436e+00 ||r(i)||/||b|| 6.084523500697e-03
  3 KSP unpreconditioned resid norm 1.186602047841e-01 true resid norm 1.186602047842e-01 ||r(i)||/||b|| 5.168934214501e-04
  4 KSP unpreconditioned resid norm 4.024299026826e-03 true resid norm 4.024299026840e-03 ||r(i)||/||b|| 1.753017110248e-05
  5 KSP unpreconditioned resid norm 4.505999953469e-04 true resid norm 4.505999953444e-04 ||r(i)||/||b|| 1.962849918577e-06
  6 KSP unpreconditioned resid norm 3.305137641065e-05 true resid norm 3.305137641716e-05 ||r(i)||/||b|| 1.439744611176e-07
| Refinement       |           1 |
| :--------------- | ----------: |
| Number of cells  |           8 |
| Number of nodes  |          27 |
| Number of DOFs   |          81 |
| Number of procs  |           1 |
| Nb of DOFs/proc  |          81 |
| Build mesh       |        0.21 |
| Model            |        0.01 |
| Material         |        0.04 |
| Boundary conditions |        0.01 |
| Create matrix    |        0.02 |
| Numbering        |        0.02 |
| Assembly         |        0.02 |
| Build RHS        |        0.02 |
| Factorize        |        0.01 |
| Solve            |        0.00 |
| Total            |        0.34 |

.. _stg1_txt72
# ----------------------------------------------------------------------------------------------
# Commande #0015 de /fsx/home/etud8-4/aster/install/mpi/lib64/aster/code_aster/CodeCommands/fin.py,
ligne 72
FIN(INFO_RESU='NON',
    RETASSAGE='NON')

No database in results, objects not saved on processor #0
****************************************************************************************************************************************************************
***                                WIDEN YOUR WINDOW TO 160 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document                                 ***
****************************************************************************************************************************************************************

------------------------------------------------------------------ PETSc Performance Summary: ------------------------------------------------------------------

petsc_aster on a  named c8g-st-c8g-24xlarge-1 with 1 processor, by etud8-4 Fri Jan 24 16:35:35 2025
Using 1 OpenMP threads
Using Petsc Release Version 3.20.5, unknown 

                         Max       Max/Min     Avg       Total
Time (sec):           3.561e-01     1.000   3.561e-01
Objects:              0.000e+00     0.000   0.000e+00
Flops:                5.005e+05     1.000   5.005e+05  5.005e+05
Flops/sec:            1.405e+06     1.000   1.405e+06  1.405e+06
MPI Msg Count:        0.000e+00     0.000   0.000e+00  0.000e+00
MPI Msg Len (bytes):  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 3.5612e-01 100.0%  5.0050e+05 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         33 1.0 2.1793e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
BuildTwoSidedF        16 1.0 2.6278e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult               48 1.0 1.1779e-04 1.0 2.92e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0 58  0  0  0   0 58  0  0  0  2483
MatMultAdd             6 1.0 8.4070e-06 1.0 1.02e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  1208
MatMultTranspose       6 1.0 1.4969e-05 1.0 1.02e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0   678
MatSolve               6 1.0 4.2880e-06 1.0 1.66e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   386
MatLUFactorSym         1 1.0 1.1070e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         1 1.0 6.3910e-06 1.0 1.09e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   170
MatConvert             1 1.0 3.3475e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatScale               2 1.0 5.2050e-06 1.0 1.69e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   325
MatResidual            6 1.0 1.5840e-05 1.0 3.70e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  2339
MatAssemblyBegin      26 1.0 6.3357e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd        26 1.0 3.1906e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 1.9920e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 1.8890e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCoarsen             1 1.0 4.3898e-04 1.0 1.01e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     2
MatZeroEntries         2 1.0 3.2600e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAXPY                2 1.0 8.6773e-05 1.0 8.29e+02 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    10
MatTranspose           6 1.0 1.5545e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMatMultSym          6 1.0 1.7050e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMatMultNum          6 1.0 4.3935e-05 1.0 5.78e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0 12  0  0  0   0 12  0  0  0  1316
MatPtAPSymbolic        2 1.0 2.5955e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatPtAPNumeric         2 1.0 8.1599e-05 1.0 8.82e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0 18  0  0  0   0 18  0  0  0  1080
MatGetBrAoCol          1 1.0 0.0000e+00 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot               19 1.0 7.8560e-06 1.0 1.42e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  1803
VecNorm               30 1.0 1.1880e-05 1.0 4.83e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0   407
VecScale              21 1.0 5.0880e-06 1.0 1.70e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   334
VecCopy               27 1.0 3.9980e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                32 1.0 3.9930e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                8 1.0 2.0630e-06 1.0 1.30e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   628
VecAYPX               44 1.0 7.1760e-06 1.0 4.54e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0   632
VecAXPBYCZ            12 1.0 2.3150e-06 1.0 4.86e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2099
VecMAXPY              27 1.0 8.5020e-06 1.0 2.02e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  2382
VecAssemblyBegin       1 1.0 6.4490e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAssemblyEnd         1 1.0 1.0990e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecPointwiseMult      35 1.0 6.1330e-06 1.0 2.84e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0   462
VecScatterBegin       60 1.0 2.2601e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecScatterEnd         60 1.0 1.6664e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          14 1.0 1.4728e-05 1.0 3.39e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0   230
SFSetGraph            17 1.0 1.6690e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp               17 1.0 3.2900e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFBcastBegin           6 1.0 3.0570e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFBcastEnd             6 1.0 1.9000e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin          2 1.0 1.0743e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceEnd            2 1.0 1.2360e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack                68 1.0 5.3370e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack              68 1.0 4.8340e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               5 1.0 2.3979e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 5.5526e-04 1.0 2.75e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0 55  0  0  0   0 55  0  0  0   495
KSPGMRESOrthog        16 1.0 1.5921e-05 1.0 2.45e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  5  0  0  0   0  5  0  0  0  1542
PCSetUp_GAMG+          1 1.0 1.8505e-03 1.0 2.15e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1 43  0  0  0   1 43  0  0  0   116
 PCGAMGCreateG         1 1.0 3.5624e-04 1.0 3.09e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0     9
 GAMG Coarsen          1 1.0 4.6028e-04 1.0 1.01e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     2
  GAMG MIS/Agg         1 1.0 4.4101e-04 1.0 1.01e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     2
 PCGAMGProl            1 1.0 8.4310e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  GAMG Prol-col        1 1.0 2.4100e-07 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  GAMG Prol-lift       1 1.0 5.9859e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
 PCGAMGOptProl         1 1.0 5.8047e-04 1.0 1.23e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0 25  0  0  0   0 25  0  0  0   212
  GAMG smooth          1 1.0 1.4518e-04 1.0 3.92e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  8  0  0  0   0  8  0  0  0   270
 PCGAMGCreateL         1 1.0 2.0930e-04 1.0 8.72e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0 17  0  0  0   0 17  0  0  0   417
  GAMG PtAP            1 1.0 2.0768e-04 1.0 8.72e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0 17  0  0  0   0 17  0  0  0   420
PCGAMG Gal l00         1 1.0 2.0623e-04 1.0 8.72e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0 17  0  0  0   0 17  0  0  0   423
PCGAMG Opt l00         1 1.0 1.1178e-04 1.0 3.70e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0   331
PCSetUp                2 1.0 1.9230e-03 1.0 2.16e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1 43  0  0  0   1 43  0  0  0   112
PCSetUpOnBlocks        6 1.0 7.2255e-05 1.0 1.09e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    15
PCApply                6 1.0 2.4016e-04 1.0 1.80e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0 36  0  0  0   0 36  0  0  0   749
------------------------------------------------------------------------------------------------------------------------

Object Type          Creations   Destructions. Reports information only for process 0.

--- Event Stage 0: Main Stage

           Container    11              7
              Matrix    54             39
      Matrix Coarsen     1              1
   Matrix Null Space     1              0
              Vector   122             74
           Index Set    30             27
   Star Forest Graph    21             16
       Krylov Solver     5              1
      Preconditioner     5              1
         PetscRandom     1              1
    Distributed Mesh     2              1
     Discrete System     2              1
           Weak Form     2              1
              Viewer     1              0
========================================================================================================================
Average time to get PetscTime(): 3.69e-08
#PETSc Option Table entries:
-ksp_monitor_true_residual # (source: command line)
-log_view # (source: command line)
-pc_gamg_verbose 2 # (source: code)
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=0 --with-mpi=1 --with-ssl=0 --with-x=0 --with-64-bit-indices=0 --with-mumps-lib="-L/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/mumps-5.6.2/lib -lzmumps -ldmumps -lmumps_common -lpord -L/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/scotch-7.0.4/lib -lesmumps -lptscotch -lptscotcherr -lptscotcherrexit -lscotch -lscotcherr -lscotcherrexit -L/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/parmetis-4.0.3_aster3/lib -lparmetis" --with-mumps-include=/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/mumps-5.6.2/include --with-blaslapack-lib="-L/lib -lopenblas" --with-scalapack-lib="-L/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/scalapack-2.2.0/lib -lscalapack " --with-python=1 --with-petsc4py=1 --download-ml=/fsx/home/etud8-4/codeaster-prerequisites-20240327-oss/.build-gcc13-openblas-ompi/content/3rd/pkg-trilinos-ml-v13.2.0.tar.gz --download-sowing=/fsx/home/etud8-4/codeaster-prerequisites-20240327-oss/.build-gcc13-openblas-ompi/content/3rd/sowing_v1.1.26-p8.tar.gz --download-hypre=/fsx/home/etud8-4/codeaster-prerequisites-20240327-oss/.build-gcc13-openblas-ompi/content/3rd/hypre_v2.29.0.tar.gz --download-superlu=/fsx/home/etud8-4/codeaster-prerequisites-20240327-oss/.build-gcc13-openblas-ompi/content/3rd/SuperLU_v6.0.1.tar.gz --download-slepc=/fsx/home/etud8-4/codeaster-prerequisites-20240327-oss/.build-gcc13-openblas-ompi/content/3rd/slepc-v3.20.1.tar.gz --download-slepc-configure-arguments="--with-slepc4py --download-arpack=/fsx/home/etud8-4/codeaster-prerequisites-20240327-oss/.build-gcc13-openblas-ompi/content/3rd/arpack_3.9.0.tar.gz" --download-hpddm=/fsx/home/etud8-4/codeaster-prerequisites-20240327-oss/.build-gcc13-openblas-ompi/content/3rd/hpddm_201eecd26177f88d7bb6287251877d8013fb64d2.tar.gz --with-openmp=1 --prefix=/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/petsc-v3.20.5 CC=mpicc CXX=mpicxx FC=mpif90 CFLAGS=-Wno-narrowing CXXFLAGS=-Wno-narrowing FCFLAGS=" -fallow-argument-mismatch" LIBS="-lgomp -lz"
-----------------------------------------
Libraries compiled on 2025-01-24 13:08:27 on ip-10-0-8-45 
Machine characteristics: Linux-5.10.226-214.880.amzn2.aarch64-aarch64-with-glibc2.17
Using PETSc directory: /fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/petsc-v3.20.5
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc -Wno-narrowing -fPIC -g -O  -fopenmp 
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -Wno-unused-dummy-argument -g -O   -fopenmp   -fopenmp
-----------------------------------------

Using include paths: -I/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/petsc-v3.20.5/include -I/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/mumps-5.6.2/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/petsc-v3.20.5/lib -L/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/petsc-v3.20.5/lib -lpetsc -Wl,-rpath,/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/petsc-v3.20.5/lib -L/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/petsc-v3.20.5/lib -L/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/mumps-5.6.2/lib -L/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/scotch-7.0.4/lib -L/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/parmetis-4.0.3_aster3/lib -L/fsx/home/etud8-4/aster/20240327/gcc13-openblas-ompi/scalapack-2.2.0/lib -L/lib -Wl,-rpath,/opt/amazon/openmpi/lib64 -L/opt/amazon/openmpi/lib64 -Wl,-rpath,/tools/acfl/24.04/gcc-13.2.0_AmazonLinux-2/lib/gcc/aarch64-linux-gnu/13.2.0 -L/tools/acfl/24.04/gcc-13.2.0_AmazonLinux-2/lib/gcc/aarch64-linux-gnu/13.2.0 -Wl,-rpath,/tools/acfl/24.04/gcc-13.2.0_AmazonLinux-2/lib/gcc -L/tools/acfl/24.04/gcc-13.2.0_AmazonLinux-2/lib/gcc -Wl,-rpath,/tools/acfl/24.04/gcc-13.2.0_AmazonLinux-2/lib64 -L/tools/acfl/24.04/gcc-13.2.0_AmazonLinux-2/lib64 -Wl,-rpath,/tools/acfl/24.04/arm-linux-compiler-24.04_AmazonLinux-2/lib -L/tools/acfl/24.04/arm-linux-compiler-24.04_AmazonLinux-2/lib -Wl,-rpath,/tools/acfl/24.04/gcc-13.2.0_AmazonLinux-2/lib -L/tools/acfl/24.04/gcc-13.2.0_AmazonLinux-2/lib -Wl,-rpath,/tools/acfl/24.04/gcc-13.2.0_AmazonLinux-2/aarch64-linux-gnu/lib -L/tools/acfl/24.04/gcc-13.2.0_AmazonLinux-2/aarch64-linux-gnu/lib -lHYPRE -lzmumps -ldmumps -lmumps_common -lpord -lesmumps -lptscotch -lptscotcherr -lptscotcherrexit -lscotch -lscotcherr -lscotcherrexit -lparmetis -lscalapack -lsuperlu -lml -lopenblas -lm -ldl -lgomp -lz -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lpthread -lstdc++ -ldl -lgomp -lz
-----------------------------------------

WARNING! There are options you set that were not used!
WARNING! could be spelling mistake, etc!
There is one unused database option. It is:
Option left: name:-pc_gamg_verbose value: 2 source: code

 ╔════════════════════════════════════════════════════════════════════════════════════════════════╗
 ║ <I> <CATAMESS_89>                                                                              ║
 ║                                                                                                ║
 ║ Liste des alarmes émises lors de l'exécution du calcul.                                        ║
 ║                                                                                                ║
 ║     Les alarmes que vous avez choisies d'ignorer sont précédées de (*).                        ║
 ║     Nombre d'occurrences pour chacune des alarmes :                                            ║
 ║            aucune alarme                                                                       ║
 ╚════════════════════════════════════════════════════════════════════════════════════════════════╝

<I> <FIN> ARRET NORMAL DANS "FIN" PAR APPEL A "JEFINI".
  
 <I> <FIN> MEMOIRE JEVEUX MINIMALE REQUISE POUR L'EXECUTION :                     197.88 Mo
 <I> <FIN> MEMOIRE JEVEUX OPTIMALE REQUISE POUR L'EXECUTION :                     209.22 Mo
 <I> <FIN> MAXIMUM DE MEMOIRE UTILISEE PAR LE PROCESSUS LORS DE L'EXECUTION :     803.80 Mo
  
 <I>       FERMETURE DES BASES EFFECTUEE
  
   STATISTIQUES CONCERNANT L'ALLOCATION DYNAMIQUE :
     TAILLE CUMULEE MAXIMUM            :                  209  Mo.
     TAILLE CUMULEE LIBEREE            :                    2  Mo.
     NOMBRE TOTAL D'ALLOCATIONS        :                 6884
     NOMBRE TOTAL DE LIBERATIONS       :                 6478
     APPELS AU MECANISME DE LIBERATION :                    0
     TAILLE MEMOIRE CUMULEE RECUPEREE  :                    0  Mo.
     VOLUME DES LECTURES               :                    0  Mo.
     VOLUME DES ECRITURES              :                    0  Mo.
  
   MEMOIRE JEVEUX MINIMALE REQUISE POUR L'EXECUTION :     197.88 Mo
     - IMPOSE DE NOMBREUX ACCES DISQUE
     - RALENTIT LA VITESSE D'EXECUTION
   MEMOIRE JEVEUX OPTIMALE REQUISE POUR L'EXECUTION :     209.22 Mo
     - LIMITE LES ACCES DISQUE
     - AMELIORE LA VITESSE D'EXECUTION
   MAXIMUM DE MEMOIRE UTILISEE PAR LE PROCESSUS     :     803.80 Mo
     - COMPREND LA MEMOIRE CONSOMMEE PAR  JEVEUX, 
       LE SUPERVISEUR PYTHON, LES LIBRAIRIES EXTERNES
  
 <I>       FIN D'EXECUTION LE : VE-24-JANV-2025 16:35:35
INFO './glob.1' deleted
Deleting './glob.2': No such file or directory
INFO './vola.1' deleted
Deleting './vola.2': No such file or directory

 ********************************************************************************
 * COMMAND                  :       USER :     SYSTEM :   USER+SYS :    ELAPSED *
 ********************************************************************************
 * DEBUT                    :       0.01 :       0.12 :       0.13 :       0.13 *
 * DEFI_FICHIER             :       0.00 :       0.00 :       0.00 :       0.00 *
 * DEFI_FICHIER             :       0.00 :       0.01 :       0.01 :       0.00 *
 * CREA_MAILLAGE            :       0.00 :       0.00 :       0.00 :       0.00 *
 * AFFE_MODELE              :       0.01 :       0.01 :       0.02 :       0.01 *
 * DEFI_MATERIAU            :       0.02 :       0.00 :       0.02 :       0.03 *
 * AFFE_MATERIAU            :       0.01 :       0.00 :       0.01 :       0.01 *
 * AFFE_CHAR_CINE           :       0.01 :       0.00 :       0.01 :       0.00 *
 * AFFE_CHAR_CINE           :       0.00 :       0.00 :       0.00 :       0.01 *
 * CALC_MATR_ELEM           :       0.02 :       0.00 :       0.02 :       0.01 *
 * NUME_DDL                 :       0.01 :       0.00 :       0.01 :       0.01 *
 * ASSE_MATRICE             :       0.01 :       0.00 :       0.01 :       0.02 *
 * CREA_CHAMP               :       0.01 :       0.00 :       0.01 :       0.00 *
 * CALC_CHAR_CINE           :       0.01 :       0.00 :       0.01 :       0.02 *
 * FIN                      :       0.01 :       0.01 :       0.02 :       0.02 *
 *  . check syntax          :       0.02 :       0.01 :       0.03 :       0.02 *
 *  . fortran               :       0.02 :       0.13 :       0.15 :       0.16 *
 *  . cleanup               :       0.08 :       0.00 :       0.08 :       0.07 *
 ********************************************************************************
 * TOTAL_JOB                :       0.22 :       0.22 :       0.44 :       0.51 *
 ********************************************************************************

# Mémoire (Mo) :   803.80 /   799.80 /   209.22 /   197.88 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0015   user+syst:        0.01s (syst:        0.01s, elaps:        0.02s)
# ----------------------------------------------------------------------------------------------
End of the Code_Aster execution
Code_Aster MPI exits normally
Exited

EXECUTION_CODE_ASTER_EXIT_69964=0


execution ended (command file #1): OK

# ------------------------------------------------------------------------------
Content of /tmp/run_aster_q4b16j_k/proc.0 after execution:
.:
total 64
-rw-r--r-- 1 etud8-4 Domain Users   163 Jan 24 16:35 69964.export
-rw-r--r-- 1 etud8-4 Domain Users 15177 Jan 24 16:35 asrun.log
-rw-r--r-- 1 etud8-4 Domain Users 43270 Jan 24 16:35 fort.6
-rw-r--r-- 1 etud8-4 Domain Users     0 Jan 24 16:35 fort.8
-rw-r--r-- 1 etud8-4 Domain Users     0 Jan 24 16:35 fort.9
drwxr-xr-x 2 etud8-4 Domain Users     6 Jan 24 16:35 REPE_IN
drwxr-xr-x 2 etud8-4 Domain Users     6 Jan 24 16:35 REPE_OUT

REPE_OUT:
total 0


# ------------------------------------------------------------------------------
Execution summary
                                      cpu     system    cpu+sys    elapsed
--------------------------------------------------------------------------------
Preparation of environment           0.00       0.00       0.00       0.00
Execution of code_aster              0.57       0.46       1.03       1.94
Copying results                      0.01       0.00       0.01       0.00
--------------------------------------------------------------------------------
Total                                0.58       0.46       1.04       1.94
--------------------------------------------------------------------------------

------------------------------------------------------------
--- DIAGNOSTIC JOB : OK
------------------------------------------------------------

